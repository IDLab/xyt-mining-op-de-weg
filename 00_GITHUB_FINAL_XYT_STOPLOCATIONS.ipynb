{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XYT Mining op de weg\n",
    "\n",
    "In dit project identificeren we start- en stoplocaties op basis van XYT gegevens. Het doel van deze opdracht is het ontwikkelen van een generiek model waarbij de computer zelf stoplocaties gaat identificeren (met zo min mogelijke menselijke aannames). Aan de hand van een literatuuronderzoek, kozen wij om de stoplocaties te identificeren via de clustertechniek DBScan. Hoe deze techniek werkt, zullen we later in dit script bespreken. \n",
    "\n",
    "Als eerst voeren we een data pre-processing uit. Daarna zullen we de parameters voor de DBScan selecteren doormiddel van de Mann Whitney U-test. Wanneer de parameters zijn gekozen, voeren we DBScan uit. De clusters die daaruit volgen, plotten we via de Google Maps API. Ook plotten we de clusters via ArcGis, maar deze koppeling is niet meegenomen in dit script (voor meer details vraag Marius). \n",
    "\n",
    "## Pre-processing data\n",
    "\n",
    "De eerste stap om stoplocaties te identificeren is data pre-processing. Doormiddel van deze stap, zullen we de data voorbereiden voor de DBScan. Wij zullen nu per stap een korte uitleg geven. \n",
    "\n",
    "##### Stap 1: data samenvoegen\n",
    "We beginnen met het samenvoegen van csv-bestanden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##importeren van packages\n",
    "import pandas as pd\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import fnmatch\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import mpld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##samenvoegen van csv bestanden\n",
    "path ='path pc'\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    configfiles = [os.path.join(dirpath, f)\n",
    "        for dirpath, dirnames, files in os.walk(path)\n",
    "        for f in fnmatch.filter(files, '*.csv')]\n",
    "        \n",
    "list = []\n",
    "for file in configfiles:\n",
    "    df = pd.read_csv(file,index_col=None, header=0, usecols=[0,1,2,3])\n",
    "    list.append(df)\n",
    "    df = pd.concat(list)\n",
    "    \n",
    "##drop index + create index\n",
    "df.reset_index(inplace=True)\n",
    "df['index'] = df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stap 2: omzetten tijd\n",
    "Daarna zetten we UTC tijd om naar locale tijd (Amsterdam) en splitten we de tijdgegevens in verschillende kolommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Omzetten van UTC tijd naar locale tijd (Amsterdam) en verwijderen overbodige kolommen\n",
    "df['Time'] = pd.DatetimeIndex(df['Dt']).time\n",
    "df['Date'] = pd.DatetimeIndex(df['Dt']).date\n",
    "df['DateTime_UTC'] = df.apply(lambda r : pd.datetime.combine(r['Date'],r['Time']),1)\n",
    "df['DateTime_Local'] = df['DateTime_UTC'].dt.tz_localize('utc').dt.tz_convert('Europe/Amsterdam')\n",
    "del df['DateTime_UTC']\n",
    "del df['Time']\n",
    "del df['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitten tijdgegevens in kolommen\n",
    "df['Date'] = pd.DatetimeIndex(df['DateTime_Local']).date\n",
    "df['Time'] = pd.DatetimeIndex(df['DateTime_Local']).time\n",
    "df['DateTime'] = df.apply(lambda r : pd.datetime.combine(r['Date'],r['Time']),1)\n",
    "\n",
    "## Hieronder nog meer code om tijd op te splitten\n",
    "df['Year'] = pd.DatetimeIndex(df['DateTime']).year\n",
    "df['Month'] = pd.DatetimeIndex(df['DateTime']).month\n",
    "df['Day'] = pd.DatetimeIndex(df['DateTime']).day\n",
    "df['Weeknr'] = pd.DatetimeIndex(df['DateTime']).week\n",
    "df['Weekdag'] = pd.DatetimeIndex(df['DateTime']).weekday\n",
    "df['Hour'] = pd.DatetimeIndex(df['DateTime']).hour\n",
    "df[\"minute\"] = pd.DatetimeIndex(df[\"DateTime\"]).minute\n",
    "df[\"sec\"] = pd.DatetimeIndex(df[\"DateTime\"]).second\n",
    "\n",
    "#Verwijderen kolommen \n",
    "del df['DateTime_Local']\n",
    "del df[\"Dt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bereken datum in minuten\n",
    "Year_min = df[\"Year\"] * 365 * 24 * 60\n",
    "Month_min = df[\"Month\"] * 31 * 24 * 60\n",
    "Day_min = df[\"Day\"] * 24 * 60\n",
    "Hour_min = df[\"Hour\"] * 60\n",
    "min_min = df[\"minute\"]\n",
    "\n",
    "df[\"date_in_min\"] = Year_min + Month_min + Day_min + Hour_min + min_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stap 5: Bereken afstand tussen twee XY punten\n",
    "\n",
    "Daarna bereken we de afstand tussen twee XY punten aan de hand van de geodesics formule van Karney (2013). Deze formule zit in de package Geopy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nieuwe kolom (van a naar b) --> drop na \n",
    "df = df.rename(index=str, columns={\"Lon\": \"Lon_a\", \"Lat\": \"Lat_a\"})\n",
    "df[\"Lat_b\"] = df[\"Lat_a\"].shift(-1)\n",
    "df[\"Lon_b\"] = df[\"Lon_a\"].shift(-1)\n",
    "df = df.dropna()\n",
    "\n",
    "## Bereken afstand (van a naar b)\n",
    "def distancer_km(row):\n",
    "    coords_1 = (row['Lat_a'], row['Lon_a'])\n",
    "    coords_2 = (row['Lat_b'], row['Lon_b'])\n",
    "    return geodesic(coords_1, coords_2).km\n",
    "    #return vincenty(coords_1, coords_2).km\n",
    "\n",
    "def distancer_m(row):\n",
    "    coords_1 = (row['Lat_a'], row['Lon_a'])\n",
    "    coords_2 = (row['Lat_b'], row['Lon_b'])\n",
    "    return geodesic(coords_1, coords_2).m\n",
    "    #return vincenty(coords_1, coords_2).km\n",
    "\n",
    "df['distance_km'] = df.apply(distancer_km, axis=1)\n",
    "df['distance_m'] = df.apply(distancer_m, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stap 6: berekenen van tijd tussen twee XY punten\n",
    "Om de tijd tussen twee XY punten te berekenen gebruiken we de package Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##verander het data type\n",
    "df[\"date_a\"] = np.array(df[\"DateTime\"], dtype=\"datetime64\")\n",
    "\n",
    "##verschil in seconden tussen twee XY punten\n",
    "df[\"date_b\"] = df[\"date_a\"]\n",
    "df[\"date_b\"] = df[\"date_b\"].shift(-1)\n",
    "df[\"date_b\"] = df[\"date_b\"].dropna()\n",
    "df[\"diff\"] = df[\"date_b\"] - df[\"date_a\"]\n",
    "df[\"diff_sec\"] = df[\"diff\"].astype('timedelta64[s]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stap 7: bereken snelheid tussen twee XY punten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## meters per seconde / km per uur\n",
    "df[\"speed_ms\"] = df[\"distance_m\"]/df[\"diff_sec\"]\n",
    "df[\"speed_kmu\"] = df[\"distance_km\"]/df[\"diff_sec\"].divide(60*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stap 8: Maak csv preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"XYT_preprocessing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select parameters voor DBScan\n",
    "\n",
    "In deze stap zullen we parameters selecteren voor de DBScan. In de literatuur (zie puntje artikelen) wordt DBScan regelmatig gebruikt om stoplocaties te vinden. Met behulp van de DBScan zijn GPS-punten te groeperen in clusters (stoplocaties) en ruis (bewegende punten). Hiervoor gaat het algoritme uit van een verschil in spatial density. Binnen het algoritme zijn er 3 elementen die je kunt aanpassen. Dit zijn de variablen en de parameters epsilon (afstand tussen twee XY punten) en minimale punten binnen de epsilon.   \n",
    "\n",
    "Om de juiste parameters te selecteren, hebben wij gekeken naar het verschil in gemiddelde snelheid tussen clusters en ruis. Dit hebben wij gedaan doormiddel van het uitvoeren van de Mann Whitney U-test. Deze test laat aan de hand van de grote van de U-waarde het verschil tussen beide groepen zien. Kortom, hoe groter de U-waarde, des te groter het verschil in snelheid tussen clusters en ruis. \n",
    "\n",
    "Verder, hebben wij verschillende variablen meegenomen in de DBScan. Hieruit bleek dat een combinatie van afstand, snelheid en tijd (volgnummer) tussen twee XY punten leidde tot de meest geschikte resultaten. Dit zorgde namelijk dat die punten werden geclusterd die bij elkaar lagen qua afstand,  snelheid en tijdsvolgorde.\n",
    "\n",
    "Maar let op: het is verstandig om per opdracht te onderzoeken welke variablen/ parameters tot de juiste resultaten leiden. Het script hieronder gaat daarbij helpen!\n",
    "\n",
    "##### Articles\n",
    "https://www.sciencedirect.com/science/article/pii/S0957417417307698\n",
    "https://www.sciencedirect.com/science/article/pii/S2352146518301820\n",
    "https://www.sciencedirect.com/science/article/pii/S1877050915008741\n",
    "https://github.com/turi-code/userguide/blob/master/clustering/dbscan.md\n",
    "\n",
    "\n",
    "##### Installeren packages\n",
    "    1. link voor download packages  https://www.lfd.uci.edu/~gohlke/pythonlibs/\n",
    "    2. installeer: geopandas, GDAL, Fiona, Basemap, Shapely, Pyproj, wheel \n",
    "    3. Bestand van download naar script map anaconda \n",
    "    4. pip install bestand.whl (behalve bij geopandas, deze moet je installeren via GitHub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stap 1: Inladen packages en pre-processed data --> voor installatie issues zie hierboven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from scipy.stats import mannwhitneyu\n",
    "from sklearn.cluster import DBSCAN\n",
    "from geopy.distance import great_circle\n",
    "from shapely.geometry import MultiPoint\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inladen csv (resultaat script \"00_bereken_afstand_snelheid_tussen_XYT_punten.ipynb\")\n",
    "df = pd.read_csv('XYT_preprocessing.csv')\n",
    "df = df[:-1] #Laatste regel is NAN\n",
    "#df_org = df #Orginele df ivm merge\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stap 2: Selecteren van variablen\n",
    "\n",
    "In het algoritme van DBScan kun je variablen selecteren die de clusters gaan vormen. In deze opdracht zijn dat de lat/ lon, de snelheid en VgNr van een XY punt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schalen van XYV punten \n",
    "coords_speed = np.asarray(df[['Lat_a', 'Lon_a', \"speed_kmu\", \"VgNr\"]])\n",
    "coords = preprocessing.scale(coords_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stap 3: Selecteren van parameters\n",
    "\n",
    "Om de juiste parameters te selecteren, hebben wij een loop geschreven om verschillende combinates van parameters te testen. Om die parameters te selecteren die zorgen voor het grootste verschil in snelheid tussen clusters en ruis voeren we de Mann Whitney U-test uit. Deze resulten plotten we in een heatmap, zodat visueel wordt welke parameters het meest geschikt zijn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functie maken om tabel te creëren met resultaten DBScan, u waarden en gemiddelde snelheden\n",
    "def gemiddelde_kmu_per_eps(df, coords, eps, min_samples, speed_column=\"speed_kmu\"):\n",
    "    model = DBSCAN(eps=eps, min_samples=min_samples, algorithm=\"auto\").fit(coords)\n",
    "    cluster_labels = model.labels_\n",
    "    num_clusters = len(set(cluster_labels))\n",
    "    df[\"cluster\"] = cluster_labels\n",
    "    clusters = df.speed_kmu[df[\"cluster\"] > -1]   \n",
    "    ruis =df.speed_kmu[df[\"cluster\"] == -1]\n",
    "    u, prob = mannwhitneyu(clusters, ruis)\n",
    "    kmu_per_cluster = df.groupby('cluster')[\"speed_kmu\"].mean().reset_index(name='kmu_per_cluster')\n",
    "    mean_kmu = kmu_per_cluster[\"kmu_per_cluster\"].mean()\n",
    "    return (eps, min_samples, num_clusters, mean_kmu, u)\n",
    "\n",
    "# Bovenstaande functie uitvoeren voor 500 verschillende epsilons en 7 verschillende MinPoints\n",
    "output_list = []\n",
    "for epsilon in np.linspace(0.10, 0.5, 500):\n",
    "    for min_samples in range(1, 8, 1):\n",
    "        output = gemiddelde_kmu_per_eps(df=df, coords=coords, eps=epsilon, min_samples=min_samples)\n",
    "        output_list.append(output)\n",
    "\n",
    "# Resultaten functie naar tabel\n",
    "output_df = pd.DataFrame(output_list, columns = [\"eps\", \"min_samples\", \"NoP_cluster\", \"mean_kmu_speed\", \"MannWhitney_U\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creëer heatmap om zo de juiste parameters te kunnen selecteren. \n",
    "x = np.array(output_df[\"eps\"])\n",
    "y = np.array(output_df[\"min_samples\"])\n",
    "z = np.array(output_df[\"MannWhitney_U\"])\n",
    "results = pd.DataFrame.from_dict(np.array([x,y,z]).T)\n",
    "results.columns = ['x','y','z']\n",
    "pivotted = results.pivot('y','x','z')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "sns.heatmap(pivotted, cmap=\"RdBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print per MinPoints de EPS met de hoogste u waarde\n",
    "\n",
    "indices = results.groupby('y')['z'].idxmax; indices\n",
    "hoi = results.loc[indices]\n",
    "\n",
    "print(hoi)\n",
    "\n",
    "# results.to_csv(\"result_find_eps.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster tabel maken\n",
    "\n",
    "In deze stap zullen we de DBScan uitvoeren. Daarna zullen we de cluster verfijnen aan de hand van post-processing. In dit proces zullen we clusters die overlappen in tijd veranderen naar unieke clusters. Gezien binnen de tijden van 1 stoplocatie, niet een andere stoplocatie kan beginnen. Deze stoplocaties zijn uniek (Zie bokeh plot voor verdere uitleg). Daarna selecteren we van elk cluster het middelste XY punt. Aan dit XY Punt, kunnen we data linken zoals streetview foto's, CBS data, lijst met bedrijven, etc. Ook maken we voor de middelste XY punten een cluster tabel, zodat er een overzicht is per luster wat de gemiddelde snelheid is en het aantal punten per cluster. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stap 1: packages + data importeren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from geopy.distance import great_circle\n",
    "from shapely.geometry import MultiPoint\n",
    "from sklearn import preprocessing\n",
    "# from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.io import push_notebook, show, output_notebook, curdoc, show\n",
    "from bokeh.models import ColumnDataSource, Plot, LinearAxis, Grid\n",
    "from bokeh.models.glyphs import VBar\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inladen csv (uitkomst vorige script)\n",
    "df = pd.read_csv('XYT_preprosessing.csv')\n",
    "df = df[:-1] #Laatste regel is NAN\n",
    "df_old = df #Nodig voor merge verderop in script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stap 2: Parameters selecteren + DBScan runnen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variabelen invoeren DBScan en schalen\n",
    "coords_speed = np.asarray(df[['Lat_a', 'Lon_a', \"speed_kmu\", \"VgNr\"]])\n",
    "coords = preprocessing.scale(coords_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecteer beste parameters uit vorige stap (heatmap/ lijst beste parameters - U waarden)\n",
    "# Binnen deze opdracht is past MinSamples 2 het beste bij het gewenste resultaat. MinSamples 2 heeft niet de hoogste U waarde (niet de hoogste percision), maar vind wel de meeste/ kleinste clusters (hoogste recall).\n",
    "epsilon = 0.233868\n",
    "min_samples = 2\n",
    "\n",
    "# run DBScan \n",
    "db = DBSCAN(eps=epsilon, min_samples=min_samples, algorithm='auto').fit(coords)\n",
    "cluster_labels = db.labels_\n",
    "num_clusters = len(set(cluster_labels))\n",
    "print('Number of clusters: {}'.format(num_clusters))\n",
    "\n",
    "#Clusterlabels naar column in df\n",
    "df[\"cluster\"] = cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stap 3: raw data opslaan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge met de oude gegevens \n",
    "df = df[[\"VgNr\", \"cluster\"]]\n",
    "df_new = pd.merge(df_old, df, on=\"VgNr\", how=\"left\")\n",
    "\n",
    "# Opslaan raw data\n",
    "df_new.to_csv(\"FINAL_raw_eps023_MinSamples2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stap 4: Post-processing\n",
    "We willen dat clusters op een volgend zijn en elkaar niet overlappen qua start en stop tijd. Daarvoor voeren we een post-processing stap uit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maken van copy van df_total en sorteren op kolom clusters en tijd\n",
    "Add_clusters = df_new.copy()\n",
    "Add_clusters.sort_values(['VgNr'])\n",
    "Add_clusters.head()\n",
    "\n",
    "#Vinden van verschillen in kolom cluster_x\n",
    "Add_clusters[\"Check_overlap\"] = Add_clusters['cluster_x'].diff()\n",
    "#Clusters nieuwe clusterwaarden geven als er een verschil is met vorige rij in kolom Check_overlap.\n",
    "#Let op nieuwe clusternummers komen niet overeen met de oude clusternummers!\n",
    "Add_clusters['Cluster_new']= (Add_clusters['Check_overlap'] != 0).astype(int).cumsum()\n",
    "\n",
    "# De oorspronkelijke cluster -1 (=ruis) overnemen als None waarden in kolom Cluster_new\n",
    "Add_clusters['Cluster_new'] = np.where(Add_clusters['cluster_x'] == -1, -1, (Add_clusters['Cluster_new']))\n",
    "\n",
    "# Verwerken van de modelresultaten tot nieuwe clusters\n",
    "result_total = Add_clusters.sort_values(['Cluster_new'])\n",
    "\n",
    "df_vgnr = result_total.groupby('Cluster_new')['VgNr'].agg(['min','max']).rename(columns={'min': 'vgnr_min', 'max': 'vgnr_max'}).reset_index()\n",
    "df_dt = result_total.groupby('Cluster_new')['DateTime'].agg(['min','max']).rename(columns={'min': 'dt_min', 'max': 'dt_max'}).reset_index()\n",
    "df_concat = pd.merge(df_dt, df_vgnr, left_on='Cluster_new', right_on='Cluster_new')\n",
    "Add_clusters[\"start\"] = Add_clusters.DateTime.isin(df_concat.dt_min).astype(int)\n",
    "Add_clusters[\"end\"] = Add_clusters.DateTime.isin(df_concat.dt_max).astype(int)\n",
    "Add_clusters[\"start\"] = Add_clusters['start'].replace(0, np.nan)\n",
    "Add_clusters[\"end\"] = Add_clusters['end'].replace(0, np.nan)\n",
    "Add_clusters.sort_values(['DateTime'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stap 5: Vind middelste XY punt\n",
    "\n",
    "Om de clusters overzichtelijk te maken, selecteren wij het middelste XY punt. Aan dit punten kunnen we cluster tabel koppelen, maar ook informatie vanuit Google Maps en CBS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maak een array van lat/ lon en clusterslabels\n",
    "coords_new = np.asarray(Add_clusters[['Lat_a', 'Lon_a']])\n",
    "cluster_array = np.asarray(Add_clusters[\"Cluster_new\"])\n",
    "num_cluster_array = len(set(cluster_array))\n",
    "clusters = pd.Series([coords_new[cluster_array == n] for n in range(num_cluster_array)])\n",
    "clusters = clusters[clusters.astype(str) != '[]']\n",
    "\n",
    "#selecteer middelste XY punt\n",
    "def get_centermost_point(cluster):\n",
    "    centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y)\n",
    "    centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m)\n",
    "    return tuple(centermost_point)\n",
    "centermost_points = clusters.map(get_centermost_point)\n",
    "\n",
    "# maak dataframe\n",
    "lats, lons = zip(*centermost_points)\n",
    "rep_points = pd.DataFrame({'Lon_a':lons, 'Lat_a':lats})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stap 6: Opslaan totale dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Add_clusters.to_csv(\"FINAL_total_eps023_MinSamples2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stap 7: Cluster tabel maken\n",
    "Koppel nu een cluster tabel aan de middelste XY punten. Het cluster tabel maakt inzichtelijk wat de gemiddelde snelheid en aantal XY punten per cluster zijn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Bereken gemiddelde snelheid en aantal XY punten per cluster\n",
    "kmu_per_cluster = Add_clusters.groupby('Cluster_new')['speed_kmu'].mean().reset_index(name='kmu_per_cluster')\n",
    "NoP_per_cluster = Add_clusters.groupby('Cluster_new')[\"Cluster_new\"].count().reset_index(name='NoP_per_cluster')\n",
    "NoP_kmu_per_cluster = pd.merge(kmu_per_cluster, NoP_per_cluster, on=[\"Cluster_new\"], how=\"left\")\n",
    "\n",
    "# koppel deze resultaten aan cluster tabel\n",
    "ID_centerpoint = Add_clusters[[\"VgNr\", \"Lat_a\", \"Lon_a\", \"Lat_b\", \"Lon_b\", \"Cluster_new\"]]\n",
    "ID_centerpoint = pd.merge(rep_points, ID_centerpoint, on=[\"Lon_a\", \"Lat_a\"], how=\"left\")\n",
    "cluster_table = pd.merge(ID_centerpoint, NoP_kmu_per_cluster, on=[\"Cluster_new\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stap 8: Sla cluster tabel op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_table.to_csv(\"FINAL_NOFILTER_ClusterTable_eps023_MinSampels_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stap 9: filter cluster met gemiddelde snelheid boven 10 km/u eruit\n",
    "Wij nemen aan dat stoplocaties geen gemiddelde snelheden boven de 10 km/u hebben. Vandaar dat wij deze eruit filteren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = cluster_table[cluster_table[\"kmu_per_cluster\"] <=10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stap 10: sla gefilterde cluster tabel op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.to_csv(\"FINAL_ClusterTable_eps023_MinSamples_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Koppeling Google Maps\n",
    "\n",
    "In deze stap plotten we de gefilterde cluster tabel via Google Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stap 1: importeer dataset + packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import googlemaps\n",
    "import os\n",
    "import gmaps\n",
    "import gmaps.datasets\n",
    "key = 'google_key'\n",
    "gmaps.configure(api_key=key) # Fill in with your API key\n",
    "earthquake_df = gmaps.datasets.load_dataset_as_df('earthquakes')\n",
    "earthquake_df.head()\n",
    "\n",
    "df = pd.read_csv(\"FINAL_ClusterTable_eps023_MinSamples_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stap 2: plot kaart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecteer XY punten\n",
    "locations = df[[\"Lat_a\", \"Lon_a\"]]\n",
    "\n",
    "# Voeg nummer van punten per cluster toe voor heatmap\n",
    "NoP_cluster = df[\"NoP_per_cluster\"]\n",
    "\n",
    "#maak plot\n",
    "fig = gmaps.figure()\n",
    "fig.add_layer(gmaps.heatmap_layer(locations, weights=NoP_cluster))\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
